{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\n\\n1. Qingguo Wei 91% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/QingguoWei_desc.html\\n\\nProcessing technique:\\n\\n���� Three features are combined to perform classification task. Two of them are extracted by common spatial subspace decomposition (CSSD) and Fisher discriminant analysis (FDA), and another feature is derived from waveform mean (WM) and FDA. The classifier is support vector machines (SVM).\\n\\n\\n2. Paul Hammon 2nd 87% accuracy - classification SVM but accuracy less\\n\\n3. Michael Sapinski 3rd 86% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/MichalSapinski_desc.txt\\n\\nPreprocessing: All channels were downsampled to 25Hz.\\n\\nFeatures used: Mean values of selected channels in some periods of time,\\nmean power of selected frequency intervals for selected channels. These\\nfeatures were constructed by hand with help of visualization of\\ndifferences for the given classes.\\n\\nClassification: Few logistic regression classifiers that seemed to give\\nquite similar results on the test set but used different features.\\n\\n\\n4. Mao Dawei 3rd 86% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/MaoDawei_desc.html\\n\\nFeature: Data from every electrode was transformed to time-frequency domain by Hilbert-Huang Transform. Extract Standard deviation of every time-frequency window as feature.\\n\\nClassification method: mahalanobis distance to class center.\\n\\nProcessing Flow:\\n\\nHilbert-Huang Transform of ECoG from all electrodes.\\n\\nExtract Standard deviation of every time-frequency window (5Hz*0.2S) as feature.\\n\\nCalculated error rate by leave-one-out use train data. Select 7 electrodes (29,30,31,38,39,40,46) that has lowest error rate with single time-frequency window.\\n\\nUse Standard deviation of time-frequency domain from all 7 electrodes, single frequency band and 11 continuous time windows as feature. Calculated class center of class 1 and calss-1 from train data.\\n\\nUse mahalanobis distance to classify the test data.\\n\\n5. Liu Yang 3rd 86% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/LiuYang_desc.txt\\n\\nPreprocessing:\\n1) filter signals using bandpass filter(2.5Hz to 25Hz). \\n2) get data of 1s length as trials after the position of every cue for both training and testing data.\\n3) compute CSP(Common Spatial Pattern) using training data, apply it to both training and testing data, then compute cwt of every trials.\\n\\nClassifying:\\n1) compute two correlation matrix A(among training trials) and C(between testing trials and training trials)\\n2) construct the feature space using every row of matrix A and C as features of corresponding trials.\\n3) optimize a linear discriminant surface from a initial position decided by the training trials under a energy penalty critera, ie.\\n   min norm(Ax-b)+norm(x) \\nwhere x is weight vector of the linear discriminant function, b is vector containing expected positions of every training samples.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''TO-DO LIST'''''\n",
    "''''\n",
    "\n",
    "1. Qingguo Wei 91% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/QingguoWei_desc.html\n",
    "\n",
    "Processing technique:\n",
    "\n",
    "���� Three features are combined to perform classification task. Two of them are extracted by common spatial subspace decomposition (CSSD) and Fisher discriminant analysis (FDA), and another feature is derived from waveform mean (WM) and FDA. The classifier is support vector machines (SVM).\n",
    "\n",
    "\n",
    "2. Paul Hammon 2nd 87% accuracy - classification SVM but accuracy less\n",
    "\n",
    "3. Michael Sapinski 3rd 86% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/MichalSapinski_desc.txt\n",
    "\n",
    "Preprocessing: All channels were downsampled to 25Hz.\n",
    "\n",
    "Features used: Mean values of selected channels in some periods of time,\n",
    "mean power of selected frequency intervals for selected channels. These\n",
    "features were constructed by hand with help of visualization of\n",
    "differences for the given classes.\n",
    "\n",
    "Classification: Few logistic regression classifiers that seemed to give\n",
    "quite similar results on the test set but used different features.\n",
    "\n",
    "\n",
    "4. Mao Dawei 3rd 86% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/MaoDawei_desc.html\n",
    "\n",
    "Feature: Data from every electrode was transformed to time-frequency domain by Hilbert-Huang Transform. Extract Standard deviation of every time-frequency window as feature.\n",
    "\n",
    "Classification method: mahalanobis distance to class center.\n",
    "\n",
    "Processing Flow:\n",
    "\n",
    "Hilbert-Huang Transform of ECoG from all electrodes.\n",
    "\n",
    "Extract Standard deviation of every time-frequency window (5Hz*0.2S) as feature.\n",
    "\n",
    "Calculated error rate by leave-one-out use train data. Select 7 electrodes (29,30,31,38,39,40,46) that has lowest error rate with single time-frequency window.\n",
    "\n",
    "Use Standard deviation of time-frequency domain from all 7 electrodes, single frequency band and 11 continuous time windows as feature. Calculated class center of class 1 and calss-1 from train data.\n",
    "\n",
    "Use mahalanobis distance to classify the test data.\n",
    "\n",
    "5. Liu Yang 3rd 86% accuracy - http://www.bbci.de/competition/iii/results/tuebingen/LiuYang_desc.txt\n",
    "\n",
    "Preprocessing:\n",
    "1) filter signals using bandpass filter(2.5Hz to 25Hz). \n",
    "2) get data of 1s length as trials after the position of every cue for both training and testing data.\n",
    "3) compute CSP(Common Spatial Pattern) using training data, apply it to both training and testing data, then compute cwt of every trials.\n",
    "\n",
    "Classifying:\n",
    "1) compute two correlation matrix A(among training trials) and C(between testing trials and training trials)\n",
    "2) construct the feature space using every row of matrix A and C as features of corresponding trials.\n",
    "3) optimize a linear discriminant surface from a initial position decided by the training trials under a energy penalty critera, ie.\n",
    "   min norm(Ax-b)+norm(x) \n",
    "where x is weight vector of the linear discriminant function, b is vector containing expected positions of every training samples.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('bci_3.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2=data[\"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 8, 3000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selecting 8 electrodes as done in the text provided in 2nd best algorithm\n",
    "data_2_new=data_2[:,(29,30,31,32,37,38,39,40),:]\n",
    "data_2_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 8, 750)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subsampling by 4 \n",
    "data_2_sub=data_2_new\n",
    "data_2_subs=np.zeros((278, 8, 750))\n",
    "for i in range(0, 278):\n",
    "    for j in range(0, 8):\n",
    "        data_2_subs[i, j, :]=signal.resample(data_2_sub[i, j, :], 750)\n",
    "\n",
    "data_2_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Average Reference\n",
    "for j in range(0, 278):\n",
    "    car=np.zeros((750,))\n",
    "    for i in range(0, 8):\n",
    "        car= car + data_2_subs[j,i,:]\n",
    "\n",
    "    car=car/8\n",
    "    #car.shape\n",
    "\n",
    "    for k in range(0, 8):\n",
    "        data_2_subs[j,k,:]=data_2_subs[j,k,:]-car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for j in range(0, 278):\n",
    "    kr=data_2_subs[j,:,:]\n",
    "    \n",
    "    scaler=StandardScaler().fit(kr.T)\n",
    "    data_2_subs[j,:,:]=scaler.transform(kr.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'from sklearn.decomposition import FastICA\\nica = FastICA(n_components=8, max_iter=200)\\n\\ndata_ica=np.zeros((278, 8, 750))\\nfor i in range(0, 278):\\n    data_ica[i, :, :]=ica.fit_transform(data_2_subs[i,:,:].T).T\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=8, max_iter=200)\n",
    "\n",
    "data_ica=np.zeros((278, 8, 750))\n",
    "for i in range(0, 278):\n",
    "    data_ica[i, :, :]=ica.fit_transform(data_2_subs[i,:,:].T).T'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bandpass filter\n",
    "b, a = signal.butter(2, 0.4, 'low', analog=False)\n",
    "data_2_subs = signal.filtfilt(b, a, data_2_subs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ritwik\\anaconda3\\lib\\site-packages\\scipy\\signal\\spectral.py:1961: UserWarning: nperseg = 256 is greater than input length  = 250, using nperseg = 250\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "(6072, 278)\n"
     ]
    }
   ],
   "source": [
    "#Extracting all the features and concatenating them \n",
    "final = np.array([])\n",
    "for j in range(0, 278):\n",
    "    data_trial=data_2_subs[j,:,:].T\n",
    "    #data_trial.shape\n",
    "\n",
    "    data_trial_s1=data_trial[0:250,:]\n",
    "    #print(data_trial_s1.shape)\n",
    "    data_trial_s2=data_trial[250:500,:]\n",
    "    #print(data_trial_s2.shape)\n",
    "    data_trial_s3=data_trial[500:750,:]\n",
    "    #print(data_trial_s3.shape)\n",
    "\n",
    "    #AR Coefficients\n",
    "    import statsmodels.api as sm\n",
    "    #from statsmodels.datasets.sunspots import load\n",
    "    #data = load()\n",
    "    ARFV=np.array([])\n",
    "\n",
    "    for i in range(0, 8):\n",
    "        rho1, sigma1 = sm.regression.linear_model.burg(data_trial_s1[:,i], order=2)\n",
    "        rho2, sigma2 = sm.regression.linear_model.burg(data_trial_s2[:,i], order=2)\n",
    "        rho3, sigma3 = sm.regression.linear_model.burg(data_trial_s3[:,i], order=2)\n",
    "        ARFV=np.append(ARFV, (rho1, rho2, rho3))\n",
    "\n",
    "    #print(ARFV) \n",
    "\n",
    "    #Haar wavelet\n",
    "    import pywt\n",
    "    HWDFV=np.array([])\n",
    "    for i in range(0, 8):\n",
    "        (cA, cD) = pywt.dwt(data_trial[:,i], 'haar')\n",
    "        HWDFV=np.append(HWDFV, cA)\n",
    "\n",
    "    #Spectral Power estimates\n",
    "    SPFV=np.array([])\n",
    "    for i in range(0, 8):\n",
    "        f1, Pxx_den1 = signal.welch(data_trial_s1[:,i], 250)\n",
    "        f2, Pxx_den2 = signal.welch(data_trial_s2[:,i], 250)\n",
    "        f3, Pxx_den3 = signal.welch(data_trial_s3[:,i], 250)\n",
    "        SPFV=np.append(SPFV, (Pxx_den1, Pxx_den2, Pxx_den3))\n",
    "\n",
    "    #Concatenaton of All the feature vectors\n",
    "    concated=np.concatenate((ARFV, HWDFV, SPFV), axis=None)\n",
    "    concated=np.reshape(concated, (-1, 1))\n",
    "    if j==0:\n",
    "        final=concated\n",
    "    else:\n",
    "        final= np.hstack((final, concated))\n",
    "    print(j)\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 6072)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=final.T\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 1)\n",
      "(278,)\n"
     ]
    }
   ],
   "source": [
    "#Labels of the training data\n",
    "labels=data['events']\n",
    "print(labels.shape)\n",
    "labels=np.reshape(labels, (-1,))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an svm Classifier\n",
    "from sklearn import svm\n",
    "\n",
    "clf1 = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf1.fit(final, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8, 3000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doing the same things for the test set\n",
    "\n",
    "data_2_new_t=data['X_test'][:,(29,30,31,32,37,38,39,40),:]\n",
    "data_2_new_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8, 750)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subsampling by 4 \n",
    "data_2_sub_t=data_2_new_t\n",
    "data_2_subs_t=np.zeros((100, 8, 750))\n",
    "for i in range(0, 100):\n",
    "    for j in range(0, 8):\n",
    "        data_2_subs_t[i, j, :]=signal.resample(data_2_sub_t[i, j, :], 750)\n",
    "\n",
    "data_2_subs_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Average Reference\n",
    "for j in range(0, 100):\n",
    "    car=np.zeros((750,))\n",
    "    for i in range(0, 8):\n",
    "        car= car + data_2_subs_t[j,i,:]\n",
    "\n",
    "    car=car/8\n",
    "    #car.shape\n",
    "\n",
    "    for k in range(0, 8):\n",
    "        data_2_subs_t[j,k,:]=data_2_subs_t[j,k,:]-car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for j in range(0, 100):\n",
    "    kr=data_2_subs_t[j,:,:]\n",
    "    \n",
    "    scaler=StandardScaler().fit(kr.T)\n",
    "    data_2_subs_t[j,:,:]=scaler.transform(kr.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'ica = FastICA(n_components=8, max_iter=200)\\n\\ndata_ica_t=np.zeros((100, 8, 750))\\nfor i in range(0, 100):\\n    data_ica_t[i, :, :]=ica.fit_transform(data_2_subs_t[i,:,:].T).T\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.decomposition import FastICA\n",
    "''''ica = FastICA(n_components=8, max_iter=200)\n",
    "\n",
    "data_ica_t=np.zeros((100, 8, 750))\n",
    "for i in range(0, 100):\n",
    "    data_ica_t[i, :, :]=ica.fit_transform(data_2_subs_t[i,:,:].T).T'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bandpass filter\n",
    "b, a = signal.butter(2, 0.4, 'low', analog=False)\n",
    "data_2_subs_t = signal.filtfilt(b, a, data_2_subs_t, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ritwik\\anaconda3\\lib\\site-packages\\scipy\\signal\\spectral.py:1961: UserWarning: nperseg = 256 is greater than input length  = 250, using nperseg = 250\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "(6072, 100)\n"
     ]
    }
   ],
   "source": [
    "final_t = np.array([])\n",
    "for j in range(0 ,100):\n",
    "    data_trial=data_2_subs_t[j,:,:].T\n",
    "    #data_trial.shape\n",
    "\n",
    "    data_trial_s1=data_trial[0:250,:]\n",
    "    #print(data_trial_s1.shape)\n",
    "    data_trial_s2=data_trial[250:500,:]\n",
    "    #print(data_trial_s2.shape)\n",
    "    data_trial_s3=data_trial[500:750,:]\n",
    "    #print(data_trial_s3.shape)\n",
    "\n",
    "    #AR Coefficients\n",
    "    import statsmodels.api as sm\n",
    "    #from statsmodels.datasets.sunspots import load\n",
    "    #data = load()\n",
    "    ARFV=np.array([])\n",
    "\n",
    "    for i in range(0, 8):\n",
    "        rho1, sigma1 = sm.regression.linear_model.burg(data_trial_s1[:,i], order=2)\n",
    "        rho2, sigma2 = sm.regression.linear_model.burg(data_trial_s2[:,i], order=2)\n",
    "        rho3, sigma3 = sm.regression.linear_model.burg(data_trial_s3[:,i], order=2)\n",
    "        ARFV=np.append(ARFV, (rho1, rho2, rho3))\n",
    "\n",
    "    #print(ARFV) \n",
    "\n",
    "    #Haar wavelet\n",
    "    import pywt\n",
    "    HWDFV=np.array([])\n",
    "    for i in range(0, 8):\n",
    "        (cA, cD) = pywt.dwt(data_trial[:,i], 'haar')\n",
    "        HWDFV=np.append(HWDFV, cA)\n",
    "\n",
    "    #Spectral Power estimates\n",
    "    SPFV=np.array([])\n",
    "    for i in range(0, 8):\n",
    "        f1, Pxx_den1 = signal.welch(data_trial_s1[:,i], 250)\n",
    "        f2, Pxx_den2 = signal.welch(data_trial_s2[:,i], 250)\n",
    "        f3, Pxx_den3 = signal.welch(data_trial_s3[:,i], 250)\n",
    "        SPFV=np.append(SPFV, (Pxx_den1, Pxx_den2, Pxx_den3))\n",
    "\n",
    "    #Concatenaton of All the feature vectors\n",
    "    concated=np.concatenate((ARFV, HWDFV, SPFV), axis=None)\n",
    "    concated=np.reshape(concated, (-1, 1))\n",
    "    if j==0:\n",
    "        final_t=concated\n",
    "    else:\n",
    "        final_t= np.hstack((final_t, concated))\n",
    "    print(j)\n",
    "print(final_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6072)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_t=final_t.T\n",
    "final_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the true labels for the test set from BCI 3 website\n",
    "truelabels=np.loadtxt(\"true_labels.txt\", delimiter=\"/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the Test labels using previously trained model\n",
    "y_pred_t = clf1.predict(final_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(truelabels, y_pred_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
